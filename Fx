from __future__ import annotations

import datetime
import logging
from dataclasses import dataclass, field
from itertools import count
from typing import Sequence, Dict, Tuple, Union, Iterator, Generator, List, Optional

import numpy
import pandas

from sylo.com.bbghelper import change_source, ticker_to_eod_timedelta
from sylo.com.progress_bar import progress_bar
from sylo.datetime.common import DayCountConvention
from sylo.datetime.date import day_count_fraction, is_business_day, date_replay, datetime_replay
from sylo.datetime.fx_calendar import FxCalendar
from sylo.mdp import MarketDataProviderType, BPipeMarketDataProvider, BConMarketDataProvider
from sylo.mdp.rs_bbg import RedshiftMarketDataProvider
from sylo.models.context import Context, TotalReturn, TotalReturnIndex, AnyTime
from sylo.models.helper import resolve_reference_date
from sylo.models.interpolate import Interp1d, InterpolationMethod
from sylo.models.tc import TransactionCost

logger = logging.getLogger(__name__)
QuoteSpecType = Union[str, datetime.date, float]


class DEFAULT:
    pass


def normalise_quote_spec(x: str) -> str:
    x = x.lower()
    if x in {'spot', 'sp', 'fix', 'fixing'}:
        return '0b'
    elif x in {'sn', 's/n'}:
        return '1b'
    return x


def normalise_quote_data(raw: Dict[str, Union[float, pandas.Series]]) -> Dict[str, Union[float, pandas.Series]]:
    tenor_points: Dict[str, float] = {k: v for k, v in raw.items() if '/' not in k}
    _raw = {k: v for k, v in sorted(raw.items()) if '/' in k}
    while _raw:
        n = len(_raw)
        for k, v in _raw.copy().items():
            i, j = k.split('/')
            if i in tenor_points:
                tenor_points[j] = tenor_points[i] + _raw.pop(k)
            elif j in tenor_points:
                tenor_points[i] = tenor_points[j] - _raw.pop(k)
        if len(_raw) == n:
            logger.warning(f'failed to normalise the remaining tenor points: {_raw}')
            break
    return tenor_points


@dataclass
class Forex:
    curve_currency: Tuple[str, str]
    curve_calendar: FxCalendar
    day_count_conv: DayCountConvention
    quote_tickers: Dict[str, str]
    fixing_ticker: str = None
    live_spot_source: str = None
    live_data_source: str = None
    live_field_names: Sequence[str] = ('LAST_PRICE',)
    forward_point_size: float = 1.
    quote_data: Dict[str, float] = None
    quote_data_history: pandas.DataFrame = field(default=None, repr=False)
    quote_data_history_drop_holidays: bool = False
    quote_data_history_drop_weekends: bool = False
    quote_data_history_drop_single_point_rows: bool = True
    quote_data_history_ffill_spot_fixing: bool = True
    quote_data_history_ffill_points: bool = False
    quote_data_history_ffill_forward_outright: bool = False
    history_spot_source: str = None
    history_data_source: str = None
    history_field_names: Sequence[str] = ('BID', 'ASK', 'LAST_PRICE',)
    history_start: datetime.date = datetime.date(1990, 1, 1)
    reference_datetime: datetime.datetime = field(default_factory=datetime.datetime.now)
    interpolation_method: InterpolationMethod = InterpolationMethod.LogLinear
    transaction_cost: TransactionCost = None
    market_data_provider: MarketDataProviderType = BPipeMarketDataProvider()
    market_data_provider_use_cache: bool = True

    def get_tenor_dates(self, quote_specs: Sequence[QuoteSpecType] = None) -> Dict[QuoteSpecType, datetime.date]:
        def unpack():
            normalised_keys: Dict[str, str] = {}
            for k in quote_specs or set(j for i in self.quote_tickers for j in i.split('/')):
                if isinstance(k, str):
                    normalised_keys[k] = normalise_quote_spec(k)
                else:
                    yield k, k
            if normalised_keys:
                ref_date = resolve_reference_date(self.reference_datetime)
                cal = self.curve_calendar
                tenor_dates = cal.get_tenor_dates(quote_specs=list(normalised_keys.values()), ref_date=ref_date)
                for k, _k in normalised_keys.items():
                    yield k, tenor_dates[_k]

        if quote_specs is None:
            # sort default tenors
            return dict(sorted(unpack(), key=lambda x: x[::-1]))
        return dict(unpack())

    def get_model_time(self, quote_specs: Sequence[QuoteSpecType] = None) -> Dict[QuoteSpecType, float]:
        def unpack():
            ref_date = resolve_reference_date(self.reference_datetime)
            for k, v in self.get_tenor_dates(quote_specs=quote_specs).items():
                if isinstance(v, datetime.date):
                    v = day_count_fraction(ref_date, v, convention=self.day_count_conv)
                yield k, v

        return dict(unpack())

    def _get_quote_tickers(self, data_source: str = None, spot_source: str = None) -> Dict[str, str]:
        def unpack():
            if self.fixing_ticker:
                yield 'fixing', self.fixing_ticker
            for k, v in self.quote_tickers.items():
                source = data_source
                if spot_source and normalise_quote_spec(k) == '0b':
                    source = spot_source
                validate = False
                if isinstance(self.market_data_provider, (BPipeMarketDataProvider, BConMarketDataProvider)):
                    validate = True
                yield k, change_source(v, source=source, override=False, validate=validate)

        return dict(unpack())

    def get_live_tickers(self) -> Dict[str, str]:
        return self._get_quote_tickers(data_source=self.live_data_source, spot_source=self.live_spot_source)

    def get_history_tickers(self) -> Dict[str, str]:
        return self._get_quote_tickers(data_source=self.history_data_source, spot_source=self.history_spot_source)

    def _fetch_live_data(self) -> pandas.Series:
        tickers = self.get_live_tickers()
        sr = self.market_data_provider.bdp(securities=tickers.values(), fields=self.live_field_names).astype(float)
        sr = sr.groupby(level=0).apply(lambda x: (x.max() + x.min()) / 2)
        sr.rename({v: k for k, v in tickers.items()}, inplace=True)
        sr = sr[[i for i in tickers if i in sr]]
        return sr

    def _fetch_history_data(self) -> pandas.DataFrame:
        if not isinstance(self.history_start, datetime.date):
            raise ValueError('history_start not specified ...')
        if self.history_data_source is None:
            raise ValueError('history_data_source not specified ...')
        logger.debug(f'fetching history data -> {type(self).__name__}[{self.history_data_source}]')
        ref_datetime = datetime.datetime.now()
        tickers = self.get_history_tickers()
        frame = self.market_data_provider.bdh(
            securities=tickers.values(),
            fields=self.history_field_names,
            start_date=self.history_start,
            end_date=self.reference_datetime.date(),
            use_cache=self.market_data_provider_use_cache,
        )
        frame = frame.groupby(level=0, axis=1).apply(lambda x: x.max(axis=1).add(x.min(axis=1)).div(2))
        frame.rename({v: k for k, v in tickers.items()}, axis=1, inplace=True)
        if isinstance(self.market_data_provider,
                      (BPipeMarketDataProvider, BConMarketDataProvider, RedshiftMarketDataProvider)):
            ref_time: datetime.timedelta = ticker_to_eod_timedelta(f'_ {self.history_data_source} _')
            if self.history_data_source[1:].isdigit():  # BFIX
                frame.index += ref_time
                bfix_time = int(self.history_data_source[1:])
                if bfix_time > 170:
                    # bfix business day moves forward after 5pm local time, shift back to align with wall time
                    frame = frame.shift(-1)
            else:  # BGN
                ref_date = resolve_reference_date(ref_datetime)
                frame.index = [i + ref_time if i < ref_date else ref_datetime for i in frame.index]
        frame.sort_index(inplace=True)
        frame = frame[[i for i in tickers if i in frame]]
        return frame

    def set_quote_data(self, data: pandas.Series, reference_datetime: datetime.datetime, copy: bool = True):
        if copy:
            data = data.copy()
        data[[i for i in data.index if '/' in i]] *= self.forward_point_size
        quote_data = normalise_quote_data(dict(data.to_dict()))
        tenor_dates = self.get_tenor_dates(quote_specs=quote_data)  # purely for sorting
        # noinspection PyTypeChecker
        self.quote_data = dict(sorted(list(quote_data.items()), key=lambda x: tenor_dates[x[0]]))
        self.reference_datetime = reference_datetime

    def _get_quote_data(self, reload: bool = False) -> Dict[str, float]:
        if self.quote_data is None or reload:
            data = self._fetch_live_data()
            self.set_quote_data(data, reference_datetime=datetime.datetime.now(), copy=False)
        return self.quote_data

    def _get_interp(self, reload: bool = False) -> Interp1d:
        quote_data = self._get_quote_data(reload=reload)
        if 'fixing' in quote_data:
            quote_data = {k: v for k, v in quote_data.items() if k != 'fixing'}
        model_time = dict(sorted((v, k) for k, v in self.get_model_time(quote_specs=quote_data).items()))
        if self.interpolation_method is InterpolationMethod.LogLinear:
            model_time = {k: v for k, v in model_time.items() if k > 0}
        x = list(model_time)
        y = [quote_data[v] for v in model_time.values()]
        return Interp1d(x, y, kind=self.interpolation_method.value, copy=False, bounds_error=False,
                        fill_value=(y[0], y[-1]), assume_sorted=True)

    def get_quote_data(self, quote_specs: Sequence[QuoteSpecType] = None,
                       reload: bool = False) -> Dict[QuoteSpecType, float]:
        if quote_specs is None:
            return self._get_quote_data(reload=reload)
        interp = self._get_interp(reload=reload)
        model_time = self.get_model_time(quote_specs=quote_specs)
        yy = interp(list(model_time.values()))
        return dict(zip(model_time.keys(), yy))

    def get_implied_carry(self, quote_specs: Sequence[QuoteSpecType] = None, decay_stub: QuoteSpecType = '0b',
                          reload: bool = False) -> Dict[QuoteSpecType, float]:
        def unpack():
            ccy_base, _ = self.curve_currency
            conv = 1. if ccy_base == 'USD' else -1.
            model_time = self.get_model_time(quote_specs=quote_specs)
            quote_data = self.get_quote_data(quote_specs=[decay_stub, *model_time.values()], reload=reload)
            ref = quote_data[decay_stub]
            for k, t in model_time.items():
                fwd = quote_data[t]
                ic = numpy.log(fwd / ref) / t * conv if t else float('nan')
                yield k, ic

        return dict(unpack())

    def set_quote_data_history(self, data: pandas.DataFrame, copy: bool = True):
        if copy:
            data = data.copy()
        data[[i for i in data if '/' in i]] *= self.forward_point_size
        if self.quote_data_history_drop_holidays:
            holidays = self.curve_calendar.get_forward_holidays()
            data = data.loc[[is_business_day(i, holidays=holidays, week_mask='1111111') for i in data.index]]
        if self.quote_data_history_drop_weekends:
            data = data.loc[[is_business_day(i, week_mask=self.curve_calendar.week_mask) for i in data.index]]
        if self.quote_data_history_ffill_spot_fixing:
            # ffill missing fixing/spot
            for k, v in data.items():
                if k in ('0b', 'fixing'):
                    v.ffill(inplace=True)
        if self.quote_data_history_ffill_points:
            # ffill missing forward points
            for k, v in data.items():
                if '/' in k:
                    v.ffill(inplace=True)
        if self.quote_data_history_ffill_forward_outright:
            # ffill missing forward outright
            for k, v in data.items():
                if '/' not in k and k not in ('0b', 'fixing'):
                    v.ffill(inplace=True)
        frame = pandas.DataFrame(normalise_quote_data(dict(data)))
        frame.dropna(axis=0, how='all', inplace=True)
        if self.quote_data_history_drop_single_point_rows:
            # drop rows that have less than 2 data points, in which case the fx curve cannot be built
            cnt = frame.drop('fixing', errors='ignore', axis=1).count(axis=1)
            frame = frame.loc[cnt > 1]
        model_time = self.get_model_time(quote_specs=list(frame))  # purely for sorting
        frame = frame[sorted(frame.columns, key=lambda x: -1 if x == 'fixing' else model_time[x])]
        self.quote_data_history = frame

    def _get_quote_data_history(self, start_date: datetime.date = None, end_date: datetime.date = None,
                                holidays: Sequence[datetime.date] = None, week_mask: str = None,
                                fill_weekdays: bool = False, fill_times: Sequence[datetime.time] = None,
                                reload: bool = False) -> pandas.DataFrame:
        if self.quote_data_history is None or reload:
            data = self._fetch_history_data()
            self.set_quote_data_history(data, copy=False)
        frame: pandas.DataFrame = self.quote_data_history
        assert isinstance(frame.index, pandas.DatetimeIndex)
        if start_date or end_date:
            if isinstance(start_date, datetime.date) and not isinstance(start_date, datetime.datetime):
                start_date = start_date.isoformat()
            if isinstance(end_date, datetime.date) and not isinstance(end_date, datetime.datetime):
                end_date = end_date.isoformat()
            frame = frame.loc[start_date:end_date]
        index = frame.index
        if holidays or week_mask:
            # drop holidays and weekends
            index = list(date_replay(index, holidays=holidays, week_mask=week_mask))
        if fill_weekdays:
            # fill all weekdays
            index = list(date_replay(index, week_mask=week_mask))
        if fill_times:
            # fill intraday times
            index = list(datetime_replay(index, fill_times=fill_times))
        if index is not frame.index:
            idx = pandas.DatetimeIndex(index)
            # asof has to apply by column. DataFrame.asof cannot ignore the leading NaNs
            frame = pandas.DataFrame({k: v.asof(idx) for k, v in frame.items()})
        return frame

    def _replay_forex_history(self, frame: pandas.DataFrame, progress_bar_on: bool = True) -> Iterator[Forex]:
        raw_data = progress_bar(list(frame.iterrows())) if progress_bar_on else frame.iterrows()
        dt: pandas.Timestamp
        for dt, row in raw_data:
            quote_data = row.dropna().to_dict()
            assert quote_data
            yield Forex(
                curve_currency=self.curve_currency,
                curve_calendar=self.curve_calendar,
                day_count_conv=self.day_count_conv,
                quote_tickers=self.quote_tickers.copy(),
                fixing_ticker=self.fixing_ticker,
                live_spot_source=self.history_spot_source or self.history_data_source,
                live_data_source=self.history_data_source,
                live_field_names=self.history_field_names,
                forward_point_size=self.forward_point_size,
                quote_data=quote_data,
                quote_data_history_drop_holidays=self.quote_data_history_drop_holidays,
                quote_data_history_drop_weekends=self.quote_data_history_drop_weekends,
                quote_data_history_drop_single_point_rows=self.quote_data_history_drop_single_point_rows,
                quote_data_history_ffill_spot_fixing=self.quote_data_history_ffill_spot_fixing,
                quote_data_history_ffill_points=self.quote_data_history_ffill_points,
                quote_data_history_ffill_forward_outright=self.quote_data_history_ffill_forward_outright,
                history_spot_source=self.history_spot_source,
                history_data_source=self.history_data_source,
                history_field_names=self.history_field_names,
                history_start=self.history_start,
                reference_datetime=dt,
                interpolation_method=self.interpolation_method,
                transaction_cost=self.transaction_cost,
            )

    def replay_forex_history(self, start_date: datetime.date = None, end_date: datetime.date = None,
                             holidays: Sequence[datetime.date] = None, week_mask: str = None,
                             fill_weekdays: bool = False, fill_times: Sequence[datetime.time] = None,
                             reload: bool = False) -> Iterator[Forex]:
        frame = self._get_quote_data_history(start_date=start_date, end_date=end_date, holidays=holidays,
                                             week_mask=week_mask, fill_weekdays=fill_weekdays, fill_times=fill_times,
                                             reload=reload)
        yield from self._replay_forex_history(frame)

    def get_quote_data_history(
            self,
            quote_specs: Sequence[QuoteSpecType] = None,
            start_date: datetime.date = None,
            end_date: datetime.date = None,
            holidays: Sequence[datetime.date] = None,
            week_mask: str = None,
            fill_weekdays: bool = False,
            fill_times: Sequence[datetime.time] = None,
            reload: bool = False,
    ) -> pandas.DataFrame:
        if quote_specs is None:
            return self._get_quote_data_history(start_date=start_date, end_date=end_date, holidays=holidays,
                                                week_mask=week_mask, fill_weekdays=fill_weekdays, fill_times=fill_times,
                                                reload=reload)

        def unpack() -> Iterator[Tuple[datetime.datetime, Dict[QuoteSpecType, float]]]:
            for forex in self.replay_forex_history(start_date=start_date, end_date=end_date, holidays=holidays,
                                                   week_mask=week_mask, fill_weekdays=fill_weekdays,
                                                   fill_times=fill_times, reload=reload):
                yield forex.reference_datetime, forex.get_quote_data(quote_specs=quote_specs)

        frame = pandas.DataFrame(dict(unpack()))
        if len(frame):
            frame = frame.T[list(quote_specs)]
        return frame

    def replay_total_returns(
            self,
            tenor: QuoteSpecType,
            decay_stub: QuoteSpecType = None,
            target_notional: Union[float, pandas.Series] = 100.,
            target_notional_in_foreign_currency: bool = False,
            rebalancing_threshold: float = None,
            trade_stack_size: int = 1,
            execution_time: datetime.time = None,
            execution_hours: Sequence[Tuple[datetime.time, datetime.time]] = AnyTime,
            start_date: datetime.date = None,
            end_date: datetime.date = None,
            holidays: Sequence[datetime.date] = None,
            week_mask: str = None,
            fill_weekdays: bool = False,
            fill_times: Sequence[datetime.time] = None,
            reload: bool = False,
    ) -> Iterator[TotalReturn]:
        if isinstance(target_notional, pandas.Series) and start_date is None:
            start_date = target_notional.first_valid_index()
        replay = _replay_stack_returns(
            context=Context(
                target_notional=target_notional,
                target_notional_in_foreign_currency=target_notional_in_foreign_currency,
                rebalancing_threshold=rebalancing_threshold,
                trade_stack_size=trade_stack_size,
                execution_time=execution_time,
                execution_hours=execution_hours,
            ),
            tenor=tenor,
            decay_stub=decay_stub,
            _id=tenor,
        )
        next(replay)
        for hist in self.replay_forex_history(start_date=start_date, end_date=end_date, holidays=holidays,
                                              week_mask=week_mask, fill_weekdays=fill_weekdays, fill_times=fill_times,
                                              reload=reload):
            try:
                yield from replay.send(hist)
            except StopIteration:
                return

    def get_total_returns(
            self,
            tenor: QuoteSpecType,
            decay_stub: QuoteSpecType = None,
            target_notional: Union[float, pandas.Series] = 100.,
            target_notional_in_foreign_currency: bool = False,
            rebalancing_threshold: float = None,
            trade_stack_size: int = 1,
            execution_time: datetime.time = None,
            execution_hours: Sequence[Tuple[datetime.time, datetime.time]] = AnyTime,
            start_date: datetime.date = None,
            end_date: datetime.date = None,
            holidays: Sequence[datetime.date] = None,
            week_mask: str = None,
            fill_weekdays: bool = False,
            fill_times: Sequence[datetime.time] = None,
            reload: bool = False,
            transaction_cost: Optional[TransactionCost] = DEFAULT,
    ) -> TotalReturnIndex:
        """
        Calculate total returns of specific rolling tenor.

        :param tenor: str, target roll tenor.
            For example, tenor='1m' would roll the underlying contract to the next 1-month settlement date whenever it
                is triggered a roll.

        :param decay_stub: str, decay tenor.
            For example, '0b' means the contract will be rolled when the current contract is aged to the spot
                settlement date.

        :param target_notional: float or pandas.Series, target USD notional in the foreign currency.
            For example, +100 means it will be long 100 USD worth of the foreign currency. When target_notional is a
                pandas.Series with a pandas.DatetimeIndex, it would get the target_notional as of the specific
                datetime.
            The target_notional timestamps do not need to be aligned to the total return index; the internal logics
                will handle that correctly.

        :param target_notional_in_foreign_currency: bool. When True, target_notional is expressed in foreign currency;
                otherwise target_notional is expressed in USD amount.

        :param rebalancing_threshold: float, the absolute deviation from the target notional before it triggers a
                rebalancing action.
            For example, suppose target_notional=+100, rebalancing_threshold=+5, it will rebalance its position to +100
                USD notional as soon as the mark-to-market notional has dropped below +95 or gone above +105.
            If rebalancing_threshold=None (default), it will not rebalance at all. If rebalancing_threshold=0., it will
                always rebalance to target exactly.

        :param trade_stack_size: int, number of stack trades for the same target tenor.
            For example, suppose tenor='1m', trade_stack_size=5, it will open 5 trades at the beginning - the first one
                expiring in 30 days, the second one expiring in 29 days, the third one expiring in 28 days and so on,
                until the stack size is reached. When any contract is aged to the decay_stub tenor, it will roll it
                accordingly. The internal logic will ensure the contracts are always rolled to different settlement
                dates.

        :param execution_time: datetime.time, the time of day by when it will trigger an execution. This option can be
                timezone aware.
            For example, execution_time=datetime.time(8, tzinfo=datetime.timezone.utc) forces the total return index to
                execute trade (roll contract/rebalancing) no sooner than 8AM UTC time.

        :param execution_hours: Sequence[Tuple[datetime.time, datetime.time]], the time ranges in which execution is
                allowed.
            For example, execution_hours=[(
                                datetime.time(7, 50, tzinfo=pytz.timezone('Europe/London')),
                                datetime.time(8, 10, tzinfo=pytz.timezone('Europe/London')),
                            )] restricts trade execution to take place between 7:50 and 8:10 London time.

        :param start_date: datetime.date, the start date it will kick off the total return index.

        :param end_date: datetime.date, the end date it will end the total return index.

        :param holidays: Sequence[datetime.date], a collection of dates that will be excluded from the total return
            index.

        :param week_mask: Sequence[str], a mask that defines which days are weekday vs weekends. By default
            week_mask='1111100', which means Mon, Tue, Wed, Thu, Fri are considered weekends while Sat and Sun are
            considered weekends. To set Sun as a weekday and Fri as a weekend, for example, set week_mask='1111001'.

        :param fill_weekdays: bool, when True forward fill all missing weekdays in the total return index.

        :param fill_times: Sequence[datetime.time], a collection of timestamps to be forward filled intraday in the
            total return index.

        :param reload: bool, a flag to trigger a history data reload. When set to True, it will hit the data source to
            reload the full history data.

        :param transaction_cost: a TransactionCost object. When set to None, no transaction cost assumption is applied.
            By default it will apply the pre-defined per currency cost assumption.

        :return: a TotalReturnIndex object.
        """
        transaction_cost = self.transaction_cost if transaction_cost is DEFAULT else transaction_cost
        return TotalReturnIndex(
            currency=''.join(self.curve_currency),
            pip_size=int(1 / self.forward_point_size),
            day_count_conv=self.day_count_conv,
            transaction_cost=transaction_cost,
            total_returns=list(self.replay_total_returns(
                tenor=tenor,
                decay_stub=decay_stub,
                target_notional=target_notional,
                target_notional_in_foreign_currency=target_notional_in_foreign_currency,
                rebalancing_threshold=rebalancing_threshold,
                trade_stack_size=trade_stack_size,
                execution_time=execution_time,
                execution_hours=execution_hours,
                start_date=start_date,
                end_date=end_date,
                holidays=holidays,
                week_mask=week_mask,
                fill_weekdays=fill_weekdays,
                fill_times=fill_times,
                reload=reload
            )),
        )

    def replay_basis_total_returns(
            self,
            basis: Tuple[QuoteSpecType, QuoteSpecType],
            decay_stub: QuoteSpecType = None,
            target_notional: Union[float, pandas.Series] = 100.,
            target_notional_in_foreign_currency: bool = False,
            rebalancing_threshold: float = None,
            execution_time: datetime.time = None,
            execution_hours: Sequence[Tuple[datetime.time, datetime.time]] = AnyTime,
            start_date: datetime.date = None,
            end_date: datetime.date = None,
            holidays: Sequence[datetime.date] = None,
            week_mask: str = None,
            fill_weekdays: bool = False,
            fill_times: Sequence[datetime.time] = None,
            reload: bool = False,
    ) -> Iterator[TotalReturn]:
        if isinstance(target_notional, pandas.Series) and start_date is None:
            start_date = target_notional.first_valid_index()
        tenor1, tenor2 = basis
        tenor_dates = self.get_tenor_dates(quote_specs=basis)  # for tenor validation only
        if tenor_dates[tenor1] >= tenor_dates[tenor2]:
            raise ValueError(f'near leg {tenor1!r} is longer than far leg {tenor2!r}, please rearrange the tenors')
        replay = _replay_sync_returns(
            context=Context(
                target_notional=target_notional,
                target_notional_in_foreign_currency=target_notional_in_foreign_currency,
                rebalancing_threshold=rebalancing_threshold,
                execution_time=execution_time,
                execution_hours=execution_hours,
            ),
            tenor_notional_tuples=[(tenor1, -1), (tenor2, 1)],
            decay_stub=decay_stub,
            _ids=basis,
        )
        next(replay)
        for hist in self.replay_forex_history(start_date=start_date, end_date=end_date, holidays=holidays,
                                              week_mask=week_mask, fill_weekdays=fill_weekdays, fill_times=fill_times,
                                              reload=reload):
            try:
                yield from replay.send(hist)
            except StopIteration:
                return

    def get_basis_total_returns(
            self,
            basis: Tuple[QuoteSpecType, QuoteSpecType],
            decay_stub: QuoteSpecType = None,
            target_notional: Union[float, pandas.Series] = 100.,
            target_notional_in_foreign_currency: bool = False,
            rebalancing_threshold: float = None,
            execution_time: datetime.time = None,
            execution_hours: Sequence[Tuple[datetime.time, datetime.time]] = AnyTime,
            start_date: datetime.date = None,
            end_date: datetime.date = None,
            holidays: Sequence[datetime.date] = None,
            week_mask: str = None,
            fill_weekdays: bool = False,
            fill_times: Sequence[datetime.time] = None,
            reload: bool = False,
            transaction_cost: Optional[TransactionCost] = DEFAULT,
    ) -> TotalReturnIndex:
        """
        Calculate total returns of specific basis tenor.

        :param basis: tuple of two str.
            For example, basis=('1m', '3m') to calculate the 1m3m basis roll. The first str specifies the tenor of the
                front leg while the second str specifies the far leg. The front leg tenor has to be shorted than the
                far leg's. A roll is triggered whenever the front leg is aged to the decay_stub tenor. The internal
                logic will make sure both legs are always rolled at the same time.

        :param decay_stub: str, see decay_stub in get_total_returns.
        :param target_notional: float or pandas.Series, see target_notional in get_total_returns.
        :param target_notional_in_foreign_currency: bool, see target_notional_in_foreign_currency in get_total_returns.
        :param rebalancing_threshold: float, see rebalancing_threshold in get_total_returns.
        :param execution_time: datetime.time, see execution_time in get_total_returns.
        :param execution_hours: Sequence[Tuple[datetime.time, datetime.time]], see execution_hours in get_total_returns.
        :param start_date: datetime.date, see start_date in get_total_returns.
        :param end_date: datetime.date, see end_date in get_total_returns.
        :param holidays: Sequence[datetime.date], see holidays in get_total_returns.
        :param week_mask: Sequence[str], see week_mask in get_total_returns.
        :param fill_weekdays: bool, see fill_weekdays in get_total_returns.
        :param fill_times: Sequence[datetime.time], see fill_times in get_total_returns.
        :param reload: bool, see reload in get_total_returns.
        :param transaction_cost: a TransactionCost object, see transaction_cost in get_total_returns.
        :return: a TotalReturnIndex object.
        """
        transaction_cost = self.transaction_cost if transaction_cost is DEFAULT else transaction_cost
        return TotalReturnIndex(
            currency=''.join(self.curve_currency),
            pip_size=int(1 / self.forward_point_size),
            day_count_conv=self.day_count_conv,
            transaction_cost=transaction_cost,
            total_returns=list(self.replay_basis_total_returns(
                basis=basis,
                decay_stub=decay_stub,
                target_notional=target_notional,
                target_notional_in_foreign_currency=target_notional_in_foreign_currency,
                rebalancing_threshold=rebalancing_threshold,
                execution_time=execution_time,
                execution_hours=execution_hours,
                start_date=start_date,
                end_date=end_date,
                holidays=holidays,
                week_mask=week_mask,
                fill_weekdays=fill_weekdays,
                fill_times=fill_times,
                reload=reload
            )),
        )


def _replay_total_returns(
        context: Context,
        tenor: QuoteSpecType,
        decay_stub: QuoteSpecType = None,
        notional_multiplier: float = 1.,
        _id: str = None,
) -> Generator[TotalReturn, Forex, None]:
    if decay_stub is None:
        decay_stub = '-1b' if tenor == '0b' else '0b'
    assert tenor != decay_stub, 'tenor and decay_stub cannot be the same...'
    prev_hist = yield None
    if not isinstance(prev_hist, Forex):
        return
    ccy_base, ccy_quote = prev_hist.curve_currency
    assert ccy_base == 'USD' or ccy_quote == 'USD', 'this model is not a USD-cross...'
    tenor_dates = prev_hist.get_tenor_dates(quote_specs=[decay_stub, tenor, '0b'])
    traded = context.settlement_adjust(tenor_dates[tenor], roll='b', calendar=prev_hist.curve_calendar)
    if traded < tenor_dates['0b']:
        # try to open a trade before spot date, terminate immediately
        return
    prev_outright = prev_hist.get_quote_data(quote_specs=[decay_stub, tenor, traded])
    prev_target_notional = target_notional = context.get_target_notional(
        asof=prev_hist.reference_datetime) * notional_multiplier
    if context.target_notional_in_foreign_currency:
        if ccy_base == 'USD':
            target_notional /= prev_outright[traded]
        else:
            target_notional *= prev_outright[traded]
    if ccy_base == 'USD':
        usd_notional = left_notional = -target_notional
        right_notional = target_notional * prev_outright[traded]
    else:
        left_notional = target_notional / prev_outright[traded]
        usd_notional = right_notional = -target_notional
    implied_carry = (prev_outright[decay_stub] - prev_outright[tenor]) * left_notional
    implied_carry /= day_count_fraction(tenor_dates[decay_stub], tenor_dates[tenor],
                                        convention=prev_hist.day_count_conv)
    if ccy_base == 'USD':
        implied_carry /= prev_outright[tenor]
    hist = yield TotalReturn(
        timestamp=prev_hist.reference_datetime,
        usd_total_return=0.,
        usd_stub_return=0.,
        usd_ref_return=0.,
        usd_implied_carry=implied_carry,
        usd_target_notional=target_notional,
        usd_turnover=abs(target_notional),
        usd_notional=usd_notional,
        left_notional=left_notional,
        right_notional=right_notional,
        next_settlement=traded,
        log_return=0.,
        tid=_id,
        hist=prev_hist,
    )
    while isinstance(hist, Forex):
        assert prev_hist.reference_datetime < hist.reference_datetime
        tenor_dates = hist.get_tenor_dates(quote_specs=[decay_stub, tenor])
        outright = hist.get_quote_data(quote_specs=[decay_stub, tenor, traded])
        # --- existing trade mark-to-market ---
        delta = (outright[traded] - prev_outright[traded]) * left_notional
        stub_delta = (outright[decay_stub] - prev_outright[decay_stub]) * left_notional
        ref_delta = (outright[tenor] - prev_outright[tenor]) * left_notional
        log_delta = numpy.log(outright[traded] / prev_outright[traded]) * target_notional
        if ccy_base == 'USD':
            delta /= outright[traded]
            stub_delta /= outright[decay_stub]
            ref_delta /= outright[tenor]
            log_delta *= -1.
            usd_notional = left_notional = -right_notional / outright[traded]
        else:
            usd_notional = right_notional = -left_notional * outright[traded]
        usd_turnover = 0.
        if context.is_in_execution_hours(asof=hist.reference_datetime):
            # --- update target notional pre-execution ---
            raw_target_notional = target_notional = context.get_target_notional(
                asof=hist.reference_datetime) * notional_multiplier
            if context.target_notional_in_foreign_currency:
                if ccy_base == 'USD':
                    target_notional /= outright[traded]
                else:
                    target_notional *= outright[traded]
            # --- roll existing trade if needed ---
            if context.shall_roll or context.shall_terminate or traded <= tenor_dates[decay_stub]:
                if hist.fixing_ticker and decay_stub == '0b':
                    # settle NDF on the fix (if fix data is available)
                    if 'fixing' in hist.quote_data:
                        fix_delta = (hist.quote_data['fixing'] - outright[traded]) * left_notional
                        if ccy_base == 'USD':
                            fix_delta /= hist.quote_data['fixing']
                        delta += fix_delta
                else:
                    # ordinary unwind
                    usd_turnover += abs(left_notional) if ccy_base == 'USD' else abs(right_notional)
                if isinstance(context.settlement_align, datetime.date):
                    next_settlement = context.settlement_align
                else:
                    next_settlement = context.settlement_adjust(tenor_dates[tenor], roll='f',
                                                                calendar=hist.curve_calendar)
                if context.shall_terminate or next_settlement <= traded:
                    # yield the last return and terminate the replay
                    yield TotalReturn(
                        timestamp=hist.reference_datetime,
                        usd_total_return=delta,
                        usd_stub_return=stub_delta,
                        usd_ref_return=ref_delta,
                        usd_turnover=usd_turnover,
                        log_return=log_delta,
                        tid=_id,
                        hist=hist,
                    )
                    return
                traded = next_settlement
                if traded not in outright:
                    outright.update(hist.get_quote_data(quote_specs=[traded]))
                if context.target_notional_in_foreign_currency:
                    target_notional = context.get_target_notional(asof=hist.reference_datetime) * notional_multiplier
                    if ccy_base == 'USD':
                        target_notional /= outright[traded]
                    else:
                        target_notional *= outright[traded]
                usd_turnover += abs(target_notional)
                if ccy_base == 'USD':
                    usd_notional = left_notional = -target_notional
                    right_notional = target_notional * outright[traded]
                else:
                    left_notional = target_notional / outright[traded]
                    usd_notional = right_notional = -target_notional
            # --- rebalance to target notional if needed ---
            elif context.shall_rebalance or (raw_target_notional != prev_target_notional) or (
                    context.rebalancing_threshold is not None and
                    abs(usd_notional + target_notional) > context.rebalancing_threshold):
                notional_delta = usd_notional + target_notional
                if ccy_base == 'USD':
                    left_notional -= notional_delta
                    right_notional += notional_delta * outright[traded]
                    usd_notional = left_notional
                else:
                    right_notional -= notional_delta
                    left_notional += notional_delta / outright[traded]
                    usd_notional = right_notional
                usd_turnover += abs(notional_delta)
            prev_target_notional = raw_target_notional
        # --- implied carry of rolled trade ---
        implied_carry = (outright[decay_stub] - outright[tenor]) * left_notional
        implied_carry /= day_count_fraction(tenor_dates[decay_stub], tenor_dates[tenor], convention=hist.day_count_conv)
        if ccy_base == 'USD':
            implied_carry /= outright[tenor]
        prev_hist, prev_outright = hist, outright
        hist = yield TotalReturn(
            timestamp=hist.reference_datetime,
            usd_total_return=delta,
            usd_stub_return=stub_delta,
            usd_ref_return=ref_delta,
            usd_implied_carry=implied_carry,
            usd_target_notional=target_notional,
            usd_turnover=usd_turnover,
            usd_notional=usd_notional,
            left_notional=left_notional,
            right_notional=right_notional,
            next_settlement=traded,
            log_return=log_delta,
            tid=_id,
            hist=hist,
        )


def _replay_sync_returns(
        context: Context,
        tenor_notional_tuples: Sequence[Tuple[QuoteSpecType, float]],
        decay_stub: QuoteSpecType = None,
        _ids: Sequence[str] = None,
) -> Generator[Sequence[TotalReturn], Forex, None]:
    if _ids is None:
        _ids = range(len(tenor_notional_tuples))

    def unpack():
        for _id, (tenor, notional_multiplier) in zip(_ids, tenor_notional_tuples):
            _replay = _replay_total_returns(
                context=context,
                tenor=tenor,
                decay_stub=decay_stub,
                notional_multiplier=notional_multiplier,
                _id=_id,
            )
            next(_replay)
            yield id(_replay), _replay

    replays: Dict[int, Generator] = dict(unpack())
    prev_settlement = None
    hist = yield None
    v: TotalReturn
    while isinstance(hist, Forex) and replays:
        vv: List[TotalReturn] = []
        for i, (k, replay) in enumerate(replays.copy().items()):
            try:
                v = replay.send(hist)
            except StopIteration:
                replays.pop(k)
            else:
                if i == 0:
                    # track rolling status of the first replay
                    if v.next_settlement != prev_settlement:
                        prev_settlement = v.next_settlement
                        context.shall_roll = True
                vv.append(v)
        context.shall_roll = False
        hist = yield vv


def _replay_stack_returns(
        context: Context,
        tenor: QuoteSpecType,
        decay_stub: QuoteSpecType = None,
        notional_multiplier: float = 1.,
        _id: str = None,
) -> Generator[Sequence[TotalReturn], Forex, None]:
    settlement_excl = context.settlement_excl
    counter = count().__next__
    replays: Dict[int, Generator] = {}
    hist = yield None
    while isinstance(hist, Forex):
        vv: List[TotalReturn] = []
        for k, replay in replays.copy().items():
            try:
                v = replay.send(hist)
            except StopIteration:
                replays.pop(k)
            else:
                settlement_excl.add(v.next_settlement)
                vv.append(v)
        while len(replays) < context.trade_stack_size:
            replay = _replay_total_returns(
                context=context,
                tenor=tenor,
                decay_stub=decay_stub,
                notional_multiplier=notional_multiplier,
                _id=f'{_id}_{counter()}' if _id else counter(),
            )
            replays[id(replay)] = replay
            next(replay)
            try:
                v = replay.send(hist)
            except StopIteration:
                break
            else:
                settlement_excl.add(v.next_settlement)
                vv.append(v)
        hist = yield vv


@dataclass
class ForexChain(Forex):
    history_chain: Sequence[Forex] = ()

    def _fetch_history_data(self) -> pandas.DataFrame:
        def unpack():
            for hist in self.history_chain:
                if hist.reference_datetime.date() < self.history_start:
                    continue
                elif hist.history_start < self.history_start:
                    hist.history_start = self.history_start
                yield hist._fetch_history_data()

        frame = pandas.concat(unpack(), axis=0)
        frame.sort_index(axis=0, inplace=True)
        return frame


if __name__ == '__main__':
    pass
